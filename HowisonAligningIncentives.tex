\documentclass{sigchi}

\widowpenalty10000
\clubpenalty10000

% Use this command to override the default ACM copyright statement (e.g. for preprints). 
% Consult the conference website for the camera-ready copyright statement.


%% EXAMPLE BEGIN -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)
% \toappear{Permission to make digital or hard copies of all or part of this work for personal or classroom use is 	granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \\
% {\emph{CHI'14}}, April 26--May 1, 2014, Toronto, Canada. \\
% Copyright \copyright~2014 ACM ISBN/14/04...\$15.00. \\
% DOI string from ACM form confirmation}
%% EXAMPLE END -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)

\toappear{Submission to WSSSPE2. Copyright James Howison 2014, licensed under CC 4.0 Attribution. \url{http://creativecommons.org/licenses/by/4.0/}}

% Arabic page numbers for submission. 
% Remove this line to eliminate page numbers for the camera ready copy
\pagenumbering{arabic}


% Load basic packages
\usepackage{balance}  % to better equalize the last page
\usepackage{graphics} % for EPS, load graphicx instead
\usepackage{times}    % comment if you want LaTeX's default font
\usepackage{url}      % llt: nicely formatted URLs

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\bf\ttfamily}}}
\makeatother
\urlstyle{leo}


% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, 
% to give it a fighting chance of not being over-written, 
% since its job is to redefine many LaTeX commands.
\usepackage[pdftex]{hyperref}
\hypersetup{
pdftitle={SIGCHI Conference Proceedings Format},
pdfauthor={LaTeX},
pdfkeywords={SIGCHI, proceedings, archival format},
bookmarksnumbered,
pdfstartview={FitH},
colorlinks,
citecolor=black,
filecolor=black,
linkcolor=black,
urlcolor=black,
breaklinks=true,
}

% create a shortcut to typeset table headings
\newcommand\tabhead[1]{\small\textbf{#1}}


% End of preamble. Here it comes the document.
\begin{document}

\title{Retract bit-rotten publications: Aligning incentives for sustaining scientific software}

\numberofauthors{1}
\author{
  \alignauthor James Howison\\
    \affaddr{University of Texas}\\
    \affaddr{1616 Guadalupe Str, Austin, TX}\\
    \email{jhowison@ischool.utexas.edu}
}

\maketitle

\begin{abstract}
How might we use our current publishing and academic reputation system to promote software sustainability? This paper seeks to provoke discussion by proposing that papers whose workflows are not kept current with the changing software ecosystem should be automatically retracted, placing the work of adjusting a purported contribution on those receiving the credit. This would be a change from the current situation where the work is placed on those seeking to use the contribution, each of which labors independently. As a provocation the proposal brings out strong reactions, and helps clarify what readers think of as worth rewarding in science. I progressively soften this proposal, eventually coming to a standpoint whereby a continuous improvement system could highlight opportunities for others to update a workflow to match changes in the software ecosystem, receiving a range of rewards from addition to a paper's author list, to in-document acknowledgment, acknowledgement on a publisher's or other website.
\end{abstract}

% \keywords{
%
% }

%\category{H.5.m.}{Information Interfaces and Presentation (e.g. HCI)}{Miscellaneous}



\section{Introduction}

Currently our publication system rewards authors at the time of publication. Ostensibly publication is the event that renders the material available for use by others. Yet there is a time-lag between publication and use of the contribution by others. For many types of contribution this time-lag is not too problematic: abstract concepts such as mathematical proofs may not decline in usefulness over this period. For contributions that rely on software, the time gap between publication and attempted use can be highly problematic creating a need for significant work by the recipient before their new science can benefit from the previous contribution. For software, at least, publication, which generates the reputational reward, is removed from use, which generates a key scientific benefit. During that gap, the software contributions "de-synchronize" due to change in the software ecosystem, an issue known as "bit rot". This paper makes an proposal (or proposals) to motivate people to undertake this work and to keep scientific software useful over time. The proposal is provocative but actionable; by provoking reaction, I hope to help readers identify what it is they really value when it comes to scientific software and consider how the work to realize that value is distributed across science.

First I discuss how scientists use the code of others, second I discuss why code becomes less useful as it de-synchronizes from the surrounding software ecosystem, third I discuss the work needed to avoid this desynchronization and highlight where that work currently falls. Fourth, I consider who might do this work and how they might be motivated to do it. I then present a range of tweaks to the current publication system that would realign. Finally I anticipate objections to these suggestions and reflect on how these reveal what is really valued in science.

\subsection{How do scientists use the code of others?}

Interviews and observations among scientists using software have highlighted a continuum of reuse of the software written by other scientists.  At one end scientists re-use code that others have written by studying it. Software is a precise (if not always clear) statement of what other scientists did; recent studies have shown that, unlike methods sections or abstract descriptions of algorithms, studying the software itself can enable scientists to understand and to be sure they've understood the methods of other scientists. Ince et al. (2012), writing in Nature, showed that without software being available, results could not be replicated; that descriptions of procedures were insufficient.

At the other end of this spectrum, scientists use the code of others to support new science, by extending the capabilities of the code to support new inquiry. Howison and Herbsleb (2011) examined the software behind three eminent papers and identified each of the pieces of code used together to produce the scientific results, calling these directly used pieces of software "complements" making up a workflow. They then worked outwards to examine the code used in term by each complement, calling those "dependencies", and drawing a map of the full stack of software drawn on to produce the results reported in the papers. The three papers used XX pieces directly but XX pieces indirectly, including many written by other scientists. By combining the code of others with new software written by themselves scientists re-use code to extend its capabilities toward new science. Re-use of this kind might involve directly editing the code of others, or it might involve writing "glue-code" to move the outputs of one piece of code to another.

I write that re-use of code forms a continuum because the need for those re-using the software to have it actually run increase from left to right (from study to extension).  Software that doesn't run at all can still be read and studied, although it is far often easier to understand code if one can run it, allowing one to test ones hypotheses about how it works with known data, or by making small changes and re-running the code. At the other end of the spectrum, efforts to re-use code through extension require the code to run, and some styles of extension require the ability to access, edit, and re-compile the source code.  In the middle of this continuum lie styles of re-use that might best be described as "parameterization," the re-use of unchanged code on new data, or small changes that expand the range of situations in which the code could be re-used.

While studying code is important, it is in extension that the potential benefits of software re-use in science become particularly apparent. Rather than spending time re-implementing from the ground up, new waves of scientific discovery can stand on the shoulders of existing work and drive science forward while saving considerable time and expense. Perhaps just as valuably, by using and re-using the same software, scientists can be more sure that they are talking about and doing the same things, allowing them to relate their results to the work of others more reliably.

Another characteristic of software re-use in science is that the work of scientists is bursty in time. Substantial periods of time can go past between one scientist preparing code and publishing a paper based on that software use, and another scientist becoming aware of the work, reading the paper, and seeking to extend the work. These lags can be years long, but probably range from a few months to a few years. Moreover, substantial periods of time can go by between uses of a scientists own software stack. In interviews with scientists they relate that they work closely with code for a period within their overall scientific projects, often transitioning from direct work with code into other, non-code intensive periods, such as data collection, paper preparation, and grant writing. In addition scientists typically have multiple projects running in parallel, some of which might involve using the same code, but quite frequently different projects either use different software code or don't use software code at all. This means that substantial periods of time can go past before scientists revisit their own workflows, periods from a couple of months to a year or more were reported. This pattern of software use is quite different to how software is re-used outside science, such as for a web application, where the code executes many times a day and is continually worked on as the primary focus of multiple people.

\subsection{What is bit-rot and how does it occur?}

During the gaps of time between the creation, publication, and re-use of software, the world changes.  The software world, at least. For any given piece of software new versions of its complements and dependencies are released, both those created directly in science and those outside science (where the pace of change is faster). This leads to a process called "bit-rot". Of course, as digital information, software does not actually rot; rather the usefulness of any specific piece of software decays. Any number of small changes can render software unable to run.  The cause can be as simple as the location of a file or the name of a folder changing, as complex as an operating system update changing some fundamental interface, or as uncontrollable as a web-service disappearing.

If the intention of software re-use is simply to study the code then bit-rot might not be such a substantial issue, after all the code is still readable. Even for parameterization or "black-box" reuse, bit-rot might only be a problem when something is unintentionally changed "underneath" the code being reused. In these situations proposals to "freeze" the software environment through virtual machines or other containers (like docker) make a great deal of sense (need a cite here): if the full state of the software environment in which the code was running can be preserved, then the code will continue to run.

Yet as the intention moves to the right in the continuum above (towards reuse through extension) the re-using scientists themselves are driven towards using current, updated, versions of the software, both immediate workflow elements and their dependencies. This occurs even if the software itself still runs! That is because newer versions not only allow for new scientific methods, but also new features, better performance, and new hardware support. Further, older versions of dependencies may not be available or the person re-using the code may not know the version originally used (making "software archeology" extremely difficult). Possibly even more importantly, communities that support the software will often decline to help with older versions, either because they no longer remember or because helping with older versions means working around already fixed issues and takes away development time that might help improve the current version. Developers are often unhappy to have users working with older versions, even if only because those users are now not providing testing and ideas for the cutting edge (cite Terry). Thus even software that continues to run can be bit-rotten, just by remaining the same in a changing software ecosystem.

\subsection{What work is needed to cope with bit-rot?}

When bit-rot sets in, a piece of software has become out of sync with the software ecosystem that surrounds it. Resynchronizing the software can be extremely frustrating work, often involving multiple rounds chasing down subtle effects of introducing new versions and finding new versions of the dependencies of your dependencies.  Not for nothing is this known as descending into ``dependency hell''. The work involves identifying desynchronization, updating components, and adjusting potentially updating code to respond to any of the changes in the code.

In theory the work of identifying desynchronization could be done prospectively, monitoring the changes in a fairly large set of complements and dependencies, and applying updates as they arrive.  Far more often, though, de-synchronization is discovered when a scientist returns to a workflow after some time and begins to extend it for new work.  If a reader has ever wondered what took their graduate student so long to get an analysis working again, it is very likely because the student was working to resynchronize the code with its surrounding ecosystem.

The burstiness of scientists work with code means that often months or even years go by before de-synchronization is discovered. Those gaps make the work significantly harder, with changes interacting with each other and being more likely to require larger jumps in versions. Larger jumps in versions not only mean that the code is likely to need more adjustment, but also that the user is even less familiar with their code and the updated codes. Moreover, it is also less likely that a support community will be able to assist, as those most active (and thus able to help) have often updated much earlier and are both less likely to remember the issues and to be motivated to help. The adage ``a stitch in time saves nine'' applies well to synchronization work; done regularly and in small steps, the work tends to be easier; it's difficulty scaling superlinearly with the time that passes since the last effort to synchronize the code.

\subsection{Who does this work now (if anyone)?}



\subsubsection{The potential user}

Currently the majority of sensing and synchronization work is done by the scientist seeking to use the software. Their motivation is relatively clear: they are motivated by the need to advance their own science and their own careers.

The downsides of having the potential user undertake synchronization work are many.  First this work is difficult and frustrating

\subsubsection{The original authors}

The original authors of the software are in an excellent position to do the synchronization work, since they know the codebase best.  Yet their motivation after the publication of an original paper that used their code is unclear.  Some may be motivated by the reputation for providing good quality software, but the lack of synchronization work suggests this is not that common. 

Even well motivated original authors can find providing synchronization work difficult. That's because it can be difficult for original authors to learn of desynchronization: often they have moved onto other work and so are not working actively with the code.  More, it is hard for them to know how others are using their code, with what parameters, and what complements. End-users at least know their own work environment; original authors have to work to know.

They are also in good position to distribute the synchronization changes to others.

\subsubsection{Third parties}

Third parties are neither motivated by maintaining the reputation of the original authors, nor by the scientific reward of the work that others want to do with the 

\subsection{The challenge}

The challenge presented by the analysis above is threefold.  We would like to identify de-synchronization of scientific software, identify which software deserves to be re-synchronized, and motivate someone to undertake the synchronization work.  The current situation places each of those three elements in the hand of individual potential end-users; it is a frustrating and costly system.

\subsection{A provocative proposal}

Any paper which is not kept synchronized is retracted.

\subsubsection{Motivating authors}
\subsubsection{Motivating others}

\subsection{Reactions and what they tell us about what is valued}









\subsection{The four meanings of replication}

Much of the discourse on software in science emphasizes the scientific goal of replication, yet the meaning of that word is somewhat unclear.  I argue that there are four distinct meanings in the discourse, each positive. First, replication implies that the scientific result in question can be repeated which implies the essential validity of the result. The second meaning is re-execution, by which the exact same software might be run to produce the exact same result. The third implies parameterization, by which the exact same software might be run with different inputs (say, data and/or configuration settings) to produce novel results. The fourth meaning invoked when we talk of replication is that a scientist might take existing work as a basis for new work, replicating in order to extend in some fashion. In these senses we see a rising contribution to the progress of science, extensibility being far more closely linked to progress than, say, mere re-execution.

Each of these practices are invoked when we speak of replication, yet seeking to achieve each has quite different implications for policy, particularly in a software context. It is entirely possible for a setup to achieve, say, re-execution without achieving parameterization nor extensibility. For example, an archived virtual machine that freezes the full software assembly (data and workflow components with their full set of dependencies) certainly achieves re-execution. If we assume that the code is available in source form, then the virtual machine likely also contributes to repeatability since it can be inspected and understood (the archiving assists in the transparency of scientific practice). Indeed it is only as a study object that such a frozen assembly of software necessarily contributes to the further meanings of replication.

Empirical studies of scientific practice show that scientists typically seek to replicate in order to extend . As such the scientist seeks to simply to run the code, but to work with the code. Needing to work with the code drives the scientist towards current, updated, versions of the software, both immediate workflow elements and their dependencies. Newer versions not only allow for new scientific methods, but also new features, better performance and new hardware support. Possibly even more importantly, communities that support the software will often decline to help with older versions, either because they no longer remember or because helping with older versions means working around already fixed issues and takes away time (and user experience) that might help improve the current version (cite usability study). 

The pattern described above also takes place at projects producing software components, each of which is also an assembly of directly used components and their dependencies. Each project works hard to keep their software in sync with surrounding components, adjusting their code to keep it current. In Howison and Herbsleb (2014)\cite{howison_sustainability} we explore the category of maintenance and describe the three types of work this requires: sensing what is changing, adjusting to keep things running, and synchronizing with other projects to minimize the downstream adjustment work of others.

In summary, then, the most scientifically useful code is that which facilitates extension by others, and a key challenge for that is motivating the work required to facilitate extension. Indeed, since the initial production of software is often sufficiently motivated by the production of papers or provision of grant money, motivating the ongoing work of keeping things running and up to date with the state of the art in technology and science is core to the question of sustainable scientific software.  How might we motivate such work?

The question is particularly difficult because we not only need to motivate it one-time but we need to motivate it for the long term, off into the relatively unknowable future. Further, we need to ensure that we are directing scientific effort towards the most needed software; an ideal system would motivate in proportion to the scientific usefulness and impact of the software in question.

\section{A proposal}

The proposal of this paper is simple and begins where others have left off in proposing improvements to publication for replication:

\begin{enumerate}
\item Papers should only be accepted if their full workflows, data and results are provided, enabling re-execution.

\item In addition authors should provide a set of regression tests and the software assemblage for each paper should be continually re-executed using a continuous integration system, such as Travis.

So far these elements cover practices already adopted (e.g., at RunMyCode.org~\cite{stodden_2012} and the Biostatistics Journal~\cite{peng_reproducible_2009}), but the key extensions are:

\item The integration system would, prior to re-executing, pull each new version of dependencies, keeping each at the most current version and then re-execute the workflow and the test suite.

\item When a software assembly fails to execute, or begins to fail tests, the accompanying paper would be retracted by the journal.
\end{enumerate}

Since a retraction is an event which threatens the reputation of its authors (being a greater negative than working on new papers is a positive), the authors of the paper will be highly motivated to investigate and correct the error, mostly likely making relatively small adjustments to keep the software assembly working. Those adjustments could then be pushed upstream and made available to everyone.

The beauty of this scheme is that it not only exposes the need for work to keep software scientifically useful, but it does so at the moment the need for the work becomes apparent, thus likely minimizing the required adjustment work.  Moreover the scheme moves the burden of sensing, adjustment, and synchronization from the many, distributed, future users and centralizes it, reducing the aggregate burden. Finally the scheme motivates those most intimately connected with the code and thus best placed to undertake the adjustment work.

In this pure form the proposal synchronizes the earning of reputation from a publication from the technical debt of the work needed to ensure the ongoing usefulness of the claimed contribution.

Yet some may well feel (and not unreasonably!) this proposal is too onerous for the authors and, by creating an unknown future burden with each and every publication, might chill the very essence of scientific communication: maintaining transparency and openness that others might learn.  Certainly the proposal fails to distinguish between those contributions that continue to be scientifically useful, given the cutting edge of science, and those whose time has passed. Thus we amend the proposal to suggest a restatement of the retraction rules:

\begin{enumerate}
	\setcounter{enumi}{3}
\item On a failed re-execution, the paper would be flagged as ``provisionally non-extendable'' and the authors informed. Each copy of the paper would prominently feature this flag. They would have a reasonable period of time (3—6 months) to correct this issue and have the flag removed. After this period (or by choice of the authors) the paper would be tagged as ``retired" and featured less prominently in the journal.
\end{enumerate}

This creates a novel tier of publications, beyond the simple triple of ``not published", ``published", and ``retracted". Authors and those responsible for tenure and/or promotion could highlight the number of papers that remain at the highest levels of contribution: those that provide an immediate base for the advancement of science by others through extension.

Yet the emphasis remains entirely on the original authors to maintain their contributions. If the system was successful and the opprobrium of having publications tagged as ``retired'' was great, then the same concerns about chilling effects would be valid. Worse, if the tag of ``retired'' was not considered dishonorable then the system would not improve things at all. 

How might this issue be turned into an opportunity to further improve things? To address this we propose yet another restatement:

\begin{enumerate}
	\setcounter{enumi}{3}
\item On a failed re-execution, the paper would be flagged as ``needing work'' and the community informed. Anyone, including but not limited to, the original authors, could then undertake this work, returning the paper to full status. Those who did so would be added to the author list of the paper.
\end{enumerate}

The beauty of this proposal is that it not only highlights the work that needs to be done, but creates a reputation market to attract those interested in undertaking the work. Moreover, since it is more valuable to be an author of a highly cited (and therefore arguably more scientifically useful) contribution, the system would provide greater rewards for more useful work, allowing effort to be concentrated on the most important contributions and allowing those of primarily historical or study value to recede to (and be marked with) appropriate status.

Yet some still might find the proposal too aggressive: after all authorship is a core value in science, one might even say sacrosanct, and the prospect of having unknown individuals added to a paper would quite likely chill participation. This reflects the fact that authorship reflects credit for many things in combination: initial ideas, hard work in data collection and only in part the focus of this proposal which is credit for providing a platform for others extensions.  

Accordingly, one might experiment with a range of rewards for sustaining the extendability of scientific papers, with being added to the author list at the large end of the scale. Other options would include being added as an acknowledgement in the downloadable paper, perhaps in a new category (as some journals do by acknowledging the work of reviewers and editors). Options also exist that don't alter the original paper (and are therefore more palatable or incremental), such as including a coversheet that lists ongoing contributions. At the small reward end would be forms of acknowledgement that exist separately from the paper, such listing on the paper's landing page. Still further would be listing credit separately from the publisher and paper entirely; that would have the downside of having limited effect, but the upside of being implementable without publisher support.

Who might be motivated to achieve rewards across this spectrum? It is hard to tell in prospect; perhaps with the right celebration and featuring of these contributions many will be attracted, particularly those who seek to make their contribution through scientific infrastructure.  Certainly, the status of ``maintainer" of packages within linux distributions comes with considerable reputation and lucrative consulting opportunities.  

Another, perhaps even more likely, source of motivated participants would be early stage researchers would be most attracted. For these potential participants these rewards could provide an entry point into the scientific reputation economy, constituting the kind of legitimate peripheral participation that creates an entry point to a community of practice~\cite{wenger_communities_1998}. Certainly these contributions would provide a new kind of quality signal while simultaneously motivating students to undertake replications. Since replications involving the nitty-gritty of the work are an excellent teaching tool (cite, cite) this proposal might creates a new contribution to education.

\subsection{Would this sustain the right unit of analysis?}

By focusing on software assemblies underlying domain scientific publications, this proposal shifts the emphasis from sustaining software packages to sustaining workflows and assemblies. Moreover it creates incentive for incremental adjustment rather than broad-based refactoring and as such might lead to increased work by sustaining (and even exacerbating) non-performant architectural choices.

It might, however, be possible to view the aggregation of adjustment work occurring within the publication maintenance system as a source to be monitored and even data-mined to identify opportunities for software package maintainers to collect and rationalize the adjustment work being undertaken. Such rationalizations would resolve issues before they show up in the publications; it would be interesting to consider how such work should be accounted for in the system. Certainly software component producers would have clear evidence of their impact: they would know where their components are contributing and, as others make adjustments in their components to sustain publications, would have evidence of impact. Finally as they merge those adjustments upstream, component producers would have evidence of the software stewardship they provide.

\section{Conclusion}

This paper is a proposal that we utilize the existing ``currency" of academic life, publications and citations, to incentivize the work needed to sustain scientific software. We go beyond existing proposals that publications include executable software assemblies; these support re-execution (and transparency that enables reimplementation) but do not adequately support the more important goal of facilitating the progress of science by facilitating extension of others results.  To achieve this we make the provocative suggestion that publications whose workflows are not maintained to work with the moving and changing software ecosystem should be retracted. We temper this by suggesting such publications be tagged and thus create an opportunity for a new type of scientific contribution, akin to package maintenance in linux systems: those that re-animate the workflow with the latest components are added to the paper, accessing credit for maintaining a scientific contribution in its best and most useful form. The proposal is systematic and actionable; moreover it just might work.

% Balancing columns in a ref list is a bit of a pain because you
% either use a hack like flushend or balance, or manually insert
% a column break.  http://www.tex.ac.uk/cgi-bin/texfaq2html?label=balance
% multicols doesn't work because we're already in two-column mode,
% and flushend isn't awesome, so I choose balance.  See this
% for more info: http://cs.brown.edu/system/software/latex/doc/balance.pdf
%
% Note that in a perfect world balance wants to be in the first
% column of the last page.
%
% If balance doesn't work for you, you can remove that and
% hard-code a column break into the bbl file right before you
% submit:
%
% http://stackoverflow.com/questions/2149854/how-to-manually-equalize-columns-
% in-an-ieee-paper-if-using-bibtex
%
% Or, just remove \balance and give up on balancing the last page.
%
\balance
% REFERENCES FORMAT
% References must be the same font size as other body text.

\bibliographystyle{acm-sigchi}
\bibliography{wsspe2-howison}
\end{document}
